{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e5ef9cc-9291-47a0-b37e-b9fef54067e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data. Initial shape: (82994, 57)\n",
      "od_claim_count\n",
      "0.0000    61784\n",
      "1.0000    15169\n",
      "2.0000     4498\n",
      "3.0000     1092\n",
      "4.0000      315\n",
      "5.0000       85\n",
      "6.0000       31\n",
      "7.0000       11\n",
      "8.0000        6\n",
      "9.0000        3\n",
      "Name: count, dtype: int64\n",
      "Start date: 2018-09-10 00:00:00\n",
      "End date:   2025-09-30 00:00:00\n",
      "\n",
      " Data Types\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 82994 entries, 0 to 82993\n",
      "Data columns (total 57 columns):\n",
      " #   Column                      Non-Null Count  Dtype         \n",
      "---  ------                      --------------  -----         \n",
      " 0   Unnamed: 0                  82994 non-null  int64         \n",
      " 1   base_policy                 82994 non-null  object        \n",
      " 2   od_claim_count              82994 non-null  float64       \n",
      " 3   car_age                     82994 non-null  float64       \n",
      " 4   experian_rank_final         77375 non-null  float64       \n",
      " 5   quarterly_service           37 non-null     float64       \n",
      " 6   pps_mapped                  81321 non-null  float64       \n",
      " 7   normalized_ncb              82994 non-null  float64       \n",
      " 8   variant_bracket_mapped      32155 non-null  float64       \n",
      " 9   daily_commute               259 non-null    float64       \n",
      " 10  cc_group_ordinal            82994 non-null  float64       \n",
      " 11  customer_age_group_ordinal  76762 non-null  float64       \n",
      " 12  hit_flag_service            33177 non-null  float64       \n",
      " 13  personal_loan_flag          71654 non-null  float64       \n",
      " 14  pucc_expiry_flag            394 non-null    float64       \n",
      " 15  embedded_red_flag           41152 non-null  float64       \n",
      " 16  is_rsa                      81692 non-null  float64       \n",
      " 17  is_ncb_protect              81692 non-null  float64       \n",
      " 18  recommended_idv             82994 non-null  float64       \n",
      " 19  ownership_count             2640 non-null   float64       \n",
      " 20  ex_showroom_price           82994 non-null  float64       \n",
      " 21  corrected_body_type         77605 non-null  object        \n",
      " 22  fuel_type                   82994 non-null  object        \n",
      " 23  transmission_type           82994 non-null  object        \n",
      " 24  device_vendor_category      65222 non-null  object        \n",
      " 25  city_mapped                 82994 non-null  object        \n",
      " 26  insurer_category            6 non-null      object        \n",
      " 27  Product_Type2               82994 non-null  object        \n",
      " 28  Plan_Type                   82994 non-null  object        \n",
      " 29  make_mapped                 82994 non-null  object        \n",
      " 30  policy_start_date           82994 non-null  datetime64[ns]\n",
      " 31  base_cover_ncb              82994 non-null  float64       \n",
      " 32  model                       82994 non-null  object        \n",
      " 33  variant_bracket             32155 non-null  object        \n",
      " 34  cc_group                    82994 non-null  object        \n",
      " 35  exposure                    82994 non-null  float64       \n",
      " 36  customer_age_group          76659 non-null  object        \n",
      " 37  customer_age                76762 non-null  float64       \n",
      " 38  exposure_calculated         82994 non-null  float64       \n",
      " 39  Product+Plan                82994 non-null  object        \n",
      " 40  cc                          82994 non-null  int64         \n",
      " 41  policy_start_year           82994 non-null  int64         \n",
      " 42  policy_start_month          82994 non-null  int64         \n",
      " 43  policy_start_period         82994 non-null  object        \n",
      " 44  user_agent                  78732 non-null  object        \n",
      " 45  make                        82994 non-null  object        \n",
      " 46  previous_insurer            6 non-null      object        \n",
      " 47  previous_policy_status      0 non-null      float64       \n",
      " 48  policy_created_on           82994 non-null  object        \n",
      " 49  registration_number         2770 non-null   object        \n",
      " 50  intermediary_id             82994 non-null  int64         \n",
      " 51  plan_id                     82994 non-null  object        \n",
      " 52  renewal                     82994 non-null  bool          \n",
      " 53  is_filtered_policy          82994 non-null  float64       \n",
      " 54  Status2                     82994 non-null  object        \n",
      " 55  previous_policy_expired     82970 non-null  object        \n",
      " 56  recommended_idv_grouped     82994 non-null  object        \n",
      "dtypes: bool(1), datetime64[ns](1), float64(25), int64(5), object(25)\n",
      "memory usage: 35.5+ MB\n",
      "\n",
      " Top Missing Value Columns\n",
      "previous_policy_status   100.0000\n",
      "previous_insurer          99.9928\n",
      "insurer_category          99.9928\n",
      "quarterly_service         99.9554\n",
      "daily_commute             99.6879\n",
      "pucc_expiry_flag          99.5253\n",
      "ownership_count           96.8190\n",
      "registration_number       96.6624\n",
      "variant_bracket           61.2562\n",
      "variant_bracket_mapped    61.2562\n",
      "hit_flag_service          60.0248\n",
      "embedded_red_flag         50.4157\n",
      "device_vendor_category    21.4136\n",
      "personal_loan_flag        13.6636\n",
      "customer_age_group         7.6331\n",
      "dtype: float64\n",
      "\n",
      " Numerical Feature Summary\n",
      "                               count         mean        std        min  \\\n",
      "Unnamed: 0                 82,994.00    42,941.46  25,098.08       0.00   \n",
      "od_claim_count             82,994.00         0.36       0.71       0.00   \n",
      "car_age                    82,994.00         0.00       0.00       0.00   \n",
      "experian_rank_final        77,375.00         5.54       1.30       2.00   \n",
      "quarterly_service              37.00         7.21      16.09       0.18   \n",
      "pps_mapped                 81,321.00         1.00       0.00       1.00   \n",
      "normalized_ncb             82,994.00         0.00       0.00       0.00   \n",
      "variant_bracket_mapped     32,155.00         2.18       0.77       1.00   \n",
      "daily_commute                 259.00        38.18      29.93       0.60   \n",
      "cc_group_ordinal           82,994.00         1.47       0.95       0.00   \n",
      "customer_age_group_ordinal 76,762.00         1.62       0.82       0.00   \n",
      "hit_flag_service           33,177.00         0.01       0.09       0.00   \n",
      "personal_loan_flag         71,654.00         0.48       0.50       0.00   \n",
      "pucc_expiry_flag              394.00         0.01       0.09       0.00   \n",
      "embedded_red_flag          41,152.00         0.68       0.47       0.00   \n",
      "is_rsa                     81,692.00         0.81       0.39       0.00   \n",
      "is_ncb_protect             81,692.00         0.00       0.00       0.00   \n",
      "recommended_idv            82,994.00   902,339.05 733,512.41       0.00   \n",
      "ownership_count             2,640.00         1.00       0.00       1.00   \n",
      "ex_showroom_price          82,994.00 1,209,182.15 585,386.76 245,500.00   \n",
      "base_cover_ncb             82,994.00         0.00       0.00       0.00   \n",
      "exposure                   82,994.00         0.89       0.24       0.00   \n",
      "customer_age               76,762.00        38.82      10.01      18.00   \n",
      "exposure_calculated        82,994.00         0.89       0.24       0.00   \n",
      "cc                         82,994.00    10,964.74  35,595.43       0.00   \n",
      "policy_start_year          82,994.00     2,023.08       1.55   2,018.00   \n",
      "policy_start_month         82,994.00         6.61       3.56       1.00   \n",
      "previous_policy_status          0.00          NaN        NaN        NaN   \n",
      "intermediary_id            82,994.00       182.30     159.95      31.00   \n",
      "is_filtered_policy         82,994.00         0.00       0.00       0.00   \n",
      "\n",
      "                                  25%          50%          75%           max  \n",
      "Unnamed: 0                  21,219.25    42,687.50    64,248.75     88,414.00  \n",
      "od_claim_count                   0.00         0.00         1.00          9.00  \n",
      "car_age                          0.00         0.00         0.00          0.00  \n",
      "experian_rank_final              5.00         6.00         7.00          7.00  \n",
      "quarterly_service                0.88         1.64         5.07         91.25  \n",
      "pps_mapped                       1.00         1.00         1.00          1.00  \n",
      "normalized_ncb                   0.00         0.00         0.00          0.00  \n",
      "variant_bracket_mapped           2.00         2.00         3.00          3.00  \n",
      "daily_commute                   18.35        32.86        47.47        280.71  \n",
      "cc_group_ordinal                 1.00         1.00         2.00          3.00  \n",
      "customer_age_group_ordinal       1.00         2.00         2.00          3.00  \n",
      "hit_flag_service                 0.00         0.00         0.00          1.00  \n",
      "personal_loan_flag               0.00         0.00         1.00          1.00  \n",
      "pucc_expiry_flag                 0.00         0.00         0.00          1.00  \n",
      "embedded_red_flag                0.00         1.00         1.00          1.00  \n",
      "is_rsa                           1.00         1.00         1.00          1.00  \n",
      "is_ncb_protect                   0.00         0.00         0.00          0.00  \n",
      "recommended_idv                  0.00   868,300.00 1,360,400.00 11,495,000.00  \n",
      "ownership_count                  1.00         1.00         1.00          1.00  \n",
      "ex_showroom_price          799,000.00 1,049,000.00 1,499,000.00 12,100,000.00  \n",
      "base_cover_ncb                   0.00         0.00         0.00          0.00  \n",
      "exposure                         0.99         1.00         1.00          1.08  \n",
      "customer_age                    32.00        37.00        44.00         80.00  \n",
      "exposure_calculated              0.99         1.00         1.00          1.00  \n",
      "cc                           1,197.00     1,199.00     1,497.00    903,253.00  \n",
      "policy_start_year            2,022.00     2,023.00     2,024.00      2,025.00  \n",
      "policy_start_month               3.00         7.00        10.00         12.00  \n",
      "previous_policy_status            NaN          NaN          NaN           NaN  \n",
      "intermediary_id                 64.00       218.00       218.00      1,213.00  \n",
      "is_filtered_policy               0.00         0.00         0.00          0.00  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.float_format', '{:,.4f}'.format)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "df = pd.read_csv(\"car_age_0_updated_dataset.csv\", low_memory=False)\n",
    "print(f\"Initial shape: {df.shape}\")\n",
    "\n",
    "counts = df['od_claim_count'].value_counts().sort_index()\n",
    "print(counts)\n",
    "\n",
    "df['policy_start_date'] = pd.to_datetime(df['policy_start_date'])\n",
    "start_date = df['policy_start_date'].min()\n",
    "end_date = df['policy_start_date'].max()\n",
    "\n",
    "print(f\"Start date: {start_date}\")\n",
    "print(f\"End date:   {end_date}\")\n",
    "\n",
    "print(\"\\n Data Types\")\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    df.info()\n",
    "\n",
    "print(\"\\n Top Missing Value Columns\")\n",
    "missing_percent = (df.isnull().sum() / len(df)) * 100\n",
    "print(missing_percent.sort_values(ascending=False).head(15))\n",
    "\n",
    "print(\"\\n Numerical Feature Summary\")\n",
    "with pd.option_context('display.max_rows', None, 'display.float_format', '{:,.2f}'.format):\n",
    "    print(df.describe(include='number').transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b32ad1-a377-40d1-9c42-dd205bcfe8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after dropping columns: (82994, 31)\n",
      "Renamed columns:\n",
      "  'Product+Plan'  =>  'Product_Plan'\n",
      "\n",
      "Final Features for Model Training (28)\n",
      "\n",
      "Numerical Features (13):\n",
      "  - experian_rank_final\n",
      "  - variant_bracket_mapped\n",
      "  - cc_group_ordinal\n",
      "  - customer_age_group_ordinal\n",
      "  - hit_flag_service\n",
      "  - personal_loan_flag\n",
      "  - embedded_red_flag\n",
      "  - is_rsa\n",
      "  - ex_showroom_price\n",
      "  - customer_age\n",
      "  - exposure_calculated\n",
      "  - cc\n",
      "  - intermediary_id\n",
      "\n",
      "Categorical Features (15):\n",
      "  - corrected_body_type\n",
      "  - fuel_type\n",
      "  - transmission_type\n",
      "  - city_mapped\n",
      "  - Product_Type2\n",
      "  - make_mapped\n",
      "  - model\n",
      "  - variant_bracket\n",
      "  - cc_group\n",
      "  - customer_age_group\n",
      "  - Product_Plan\n",
      "  - make\n",
      "  - policy_created_on\n",
      "  - Status2\n",
      "  - recommended_idv_grouped\n"
     ]
    }
   ],
   "source": [
    "# 1. Preprocessing + Feature Engineering\n",
    "\n",
    "# 1a. Drop columns\n",
    "missing_percent = (df.isnull().sum() / len(df)) * 100\n",
    "cols_mostly_empty = missing_percent[missing_percent > 70].index.tolist()\n",
    "unique_counts = df.nunique()\n",
    "cols_single_value = unique_counts[unique_counts == 1].index.tolist()\n",
    "\n",
    "cols_to_drop = list(set(cols_mostly_empty + cols_single_value))\n",
    "\n",
    "cols_to_drop.extend([\n",
    "    'Unnamed: 0', \n",
    "    'policy_start_period', \n",
    "    'policy_start_year', \n",
    "    'user_agent', \n",
    "    'device_vendor_category',\n",
    "    'base_policy',           \n",
    "    'recommended_idv',\n",
    "    'previous_policy_expired',\n",
    "    'policy_start_month'\n",
    "])\n",
    "\n",
    "key_cols = ['od_claim_count', 'exposure', 'policy_start_date']\n",
    "cols_to_drop = [col for col in cols_to_drop if col not in key_cols]\n",
    "\n",
    "df_cleaned = df.drop(columns=cols_to_drop)\n",
    "print(f\"Shape after dropping columns: {df_cleaned.shape}\")\n",
    "\n",
    "# 1b. Sanitize Feature Names\n",
    "original_cols = df_cleaned.columns.tolist()\n",
    "sanitized_cols = [re.sub(r'[^A-Za-z0-9_]+', '_', col) for col in original_cols]\n",
    "df_cleaned.columns = sanitized_cols\n",
    "\n",
    "renamed_cols_dict = {orig: new for orig, new in zip(original_cols, sanitized_cols) if orig != new}\n",
    "if renamed_cols_dict:\n",
    "    print(\"Renamed columns:\")\n",
    "    for orig, new in renamed_cols_dict.items():\n",
    "        print(f\"  '{orig}'  =>  '{new}'\")\n",
    "\n",
    "# 1c. Handle date Column\n",
    "DATE_COL_NAME = 'policy_start_date'\n",
    "df_cleaned[DATE_COL_NAME] = pd.to_datetime(df_cleaned[DATE_COL_NAME])\n",
    "\n",
    "# 1d. Define Feature, Target, and Weight\n",
    "TARGET = 'od_claim_count'\n",
    "WEIGHT = 'exposure'\n",
    "DATE_COL = 'policy_start_date'\n",
    "features = [col for col in df_cleaned.columns if col not in [TARGET, WEIGHT, DATE_COL]]\n",
    "\n",
    "# 1e. Handle Categorical Features & NaNs\n",
    "categorical_features = df_cleaned[features].select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "for col in categorical_features:\n",
    "    df_cleaned[col] = df_cleaned[col].astype('category')\n",
    "\n",
    "# 1f. Print Feature Lists\n",
    "print(f\"\\nFinal Features for Model Training ({len(features)})\")\n",
    "numerical_features = df_cleaned[features].select_dtypes(include=np.number).columns.tolist()\n",
    "print(f\"\\nNumerical Features ({len(numerical_features)}):\")\n",
    "for col in numerical_features:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(f\"\\nCategorical Features ({len(categorical_features)}):\")\n",
    "for col in categorical_features:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "174c5110-344d-424f-a4ad-59adc07a24cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting data into new Train, Validation, and OOT sets...\n",
      "  Train set shape: (36558, 31) (Dates: 2023-01-01 to 2024-08-31)\n",
      "  Test set shape: (8614, 31) (Dates: 2024-09-01 to 2024-12-31)\n",
      "  OOT set shape:   (5809, 31) (Dates: 2025-01-01 to 2025-03-31)\n"
     ]
    }
   ],
   "source": [
    "# 2. Time-Based Data Splitting\n",
    "print(\"\\nSplitting data into new Train, Validation, and OOT sets...\")\n",
    "\n",
    "TRAIN_START = '2023-01-01'\n",
    "TRAIN_END = '2024-08-31'\n",
    "VALID_START = '2024-09-01'\n",
    "VALID_END = '2024-12-31'\n",
    "OOT_START = '2025-01-01'\n",
    "OOT_END = '2025-03-31'\n",
    "\n",
    "# Create the sets\n",
    "train_df = df_cleaned[\n",
    "    (df_cleaned[DATE_COL] >= TRAIN_START) &\n",
    "    (df_cleaned[DATE_COL] <= TRAIN_END)\n",
    "].copy()\n",
    "\n",
    "test_df = df_cleaned[\n",
    "    (df_cleaned[DATE_COL] >= VALID_START) &\n",
    "    (df_cleaned[DATE_COL] <= VALID_END)\n",
    "].copy()\n",
    "\n",
    "oot_df = df_cleaned[\n",
    "    (df_cleaned[DATE_COL] >= OOT_START) &\n",
    "    (df_cleaned[DATE_COL] <= OOT_END)\n",
    "].copy()\n",
    "\n",
    "print(f\"  Train set shape: {train_df.shape} (Dates: {train_df[DATE_COL].min().date()} to {train_df[DATE_COL].max().date()})\")\n",
    "print(f\"  Test set shape: {test_df.shape} (Dates: {test_df[DATE_COL].min().date()} to {test_df[DATE_COL].max().date()})\")\n",
    "print(f\"  OOT set shape:   {oot_df.shape} (Dates: {oot_df[DATE_COL].min().date()} to {oot_df[DATE_COL].max().date()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f43a2e37-d582-4afa-8272-797b4b5476de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LightGBM model (Recent Window)...\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=500, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=500\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=500, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=500\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001520 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 630\n",
      "[LightGBM] [Info] Number of data points in the train set: 36558, number of used features: 26\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=500, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=500\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Start training from score -0.909493\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[568]\tvalid_0's poisson: 0.696702\n",
      "Model training complete.\n",
      "Generating predictions...\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=500, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=500\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=500, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=500\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=500, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=500\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    }
   ],
   "source": [
    "# 3. Model Training (LightGBM Poisson Regressor)\n",
    "\n",
    "# Prepare data splits\n",
    "X_train, y_train, w_train = train_df[features], train_df[TARGET], train_df[WEIGHT]\n",
    "X_test, y_test, w_test = test_df[features], test_df[TARGET], test_df[WEIGHT]\n",
    "X_oot, y_oot, w_oot = oot_df[features], oot_df[TARGET], oot_df[WEIGHT]\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'poisson',\n",
    "    'metric': 'poisson',\n",
    "    'n_estimators': 4000,         \n",
    "    'learning_rate': 0.01,       \n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'num_leaves': 20,            \n",
    "    'min_data_in_leaf': 500,     \n",
    "    'n_jobs': -1,\n",
    "    'seed': 42,\n",
    "    'boosting_type': 'gbdt',\n",
    "}\n",
    "\n",
    "print(\"\\nTraining LightGBM model (Recent Window)...\")\n",
    "\n",
    "model = lgb.LGBMRegressor(**lgb_params)\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    sample_weight=w_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_sample_weight=[w_test],\n",
    "    eval_metric='poisson',\n",
    "    callbacks=[lgb.early_stopping(150, verbose=100)],\n",
    "    categorical_feature=categorical_features\n",
    ")\n",
    "\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Prediction\n",
    "print(\"Generating predictions...\")\n",
    "pred_train = model.predict(X_train) \n",
    "pred_test = model.predict(X_test) \n",
    "pred_oot = model.predict(X_oot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02f0a4b3-e331-4d2f-89c4-65bb369ebe53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Decile Analysis: Train Set -\n",
      "        Policies Actual IR Predicted IR   Delta\n",
      "decile                                         \n",
      "0           3656    17.29%       23.86%  -6.57%\n",
      "1           3656    22.72%       28.88%  -6.16%\n",
      "2           3656    26.84%       31.95%  -5.11%\n",
      "3           3655    31.96%       34.63%  -2.67%\n",
      "4           3656    35.85%       37.12%  -1.26%\n",
      "5           3656    39.38%       39.71%  -0.33%\n",
      "6           3655    41.79%       42.68%  -0.89%\n",
      "7           3656    50.49%       46.46%   4.02%\n",
      "8           3656    56.74%       51.88%   4.86%\n",
      "9           3656    78.99%       64.77%  14.23%\n",
      "\n",
      "Overall Actual IR:    40.2071%\n",
      "Overall Predicted IR: 40.1939%\n",
      "Mean Absolute Delta:  4.6105%\n",
      "\n",
      "- Decile Analysis: Test Set -\n",
      "        Policies Actual IR Predicted IR    Delta\n",
      "decile                                          \n",
      "0            862    23.73%       25.57%   -1.84%\n",
      "1            861    26.55%       31.70%   -5.15%\n",
      "2            861    31.82%       35.62%   -3.80%\n",
      "3            862    32.85%       38.94%   -6.08%\n",
      "4            861    34.91%       42.03%   -7.12%\n",
      "5            861    36.78%       45.20%   -8.42%\n",
      "6            862    42.77%       48.70%   -5.93%\n",
      "7            861    42.30%       53.09%  -10.79%\n",
      "8            861    51.08%       58.77%   -7.69%\n",
      "9            862    55.96%       72.71%  -16.75%\n",
      "\n",
      "Overall Actual IR:    37.5536%\n",
      "Overall Predicted IR: 44.7949%\n",
      "Mean Absolute Delta:  7.3554%\n",
      "\n",
      "- Decile Analysis: OOT Set -\n",
      "        Policies Actual IR Predicted IR    Delta\n",
      "decile                                          \n",
      "0            581    23.86%       37.14%  -13.29%\n",
      "1            581    24.07%       46.46%  -22.39%\n",
      "2            581    31.73%       51.84%  -20.11%\n",
      "3            581    31.61%       56.44%  -24.83%\n",
      "4            581    40.99%       60.71%  -19.72%\n",
      "5            580    41.63%       65.03%  -23.40%\n",
      "6            581    36.47%       70.21%  -33.74%\n",
      "7            581    46.07%       76.33%  -30.26%\n",
      "8            581    51.95%       84.93%  -32.99%\n",
      "9            581    65.31%      105.31%  -40.00%\n",
      "\n",
      "Overall Actual IR:    38.7561%\n",
      "Overall Predicted IR: 64.4413%\n",
      "Mean Absolute Delta:  26.0718%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26.07182475602491"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Decile Analysis Function\n",
    "def create_decile_analysis(y_true, y_pred_count, exposure, set_name):\n",
    "    print(f\"\\n- Decile Analysis: {set_name} Set -\")\n",
    "    \n",
    "    eval_df = pd.DataFrame({\n",
    "        'actual_count': y_true,\n",
    "        'predicted_count': y_pred_count,\n",
    "        'exposure': exposure\n",
    "    })\n",
    "    eval_df = eval_df[eval_df['exposure'] > 0].copy()\n",
    "    \n",
    "    if eval_df.empty:\n",
    "        print(f\"Warning: No data with exposure > 0 in {set_name} set. Skipping analysis.\")\n",
    "        return\n",
    "\n",
    "    eval_df['predicted_ir_record'] = eval_df['predicted_count'] / eval_df['exposure']\n",
    "    \n",
    "    try:\n",
    "        eval_df['decile'] = pd.qcut(eval_df['predicted_ir_record'], 10, labels=False, duplicates='drop')\n",
    "    except ValueError:\n",
    "        print(\"Warning: Could not create 10 unique deciles. Using fewer bins.\")\n",
    "        try:\n",
    "            eval_df['decile'] = pd.qcut(eval_df['predicted_ir_record'], 5, labels=False, duplicates='drop') # Fallback\n",
    "        except ValueError:\n",
    "             print(\"Error: Could not create any deciles. Data may be too uniform. Skipping decile table.\")\n",
    "             return\n",
    "\n",
    "    decile_groups = eval_df.groupby('decile')\n",
    "    \n",
    "    decile_summary = pd.DataFrame({\n",
    "        'Policies': decile_groups.size(),\n",
    "        'Total Exposure': decile_groups['exposure'].sum(),\n",
    "        'Actual Claims': decile_groups['actual_count'].sum(),\n",
    "        'Predicted Claims': decile_groups['predicted_count'].sum()\n",
    "    })\n",
    "    \n",
    "    decile_summary['Actual IR'] = decile_summary['Actual Claims'] / decile_summary['Total Exposure']\n",
    "    decile_summary['Predicted IR'] = decile_summary['Predicted Claims'] / decile_summary['Total Exposure']\n",
    "    decile_summary['Delta'] = decile_summary['Actual IR'] - decile_summary['Predicted IR']\n",
    "    \n",
    "    mean_abs_delta = (decile_summary['Actual IR'] - decile_summary['Predicted IR']).abs().mean() * 100\n",
    "\n",
    "    format_cols_pct = ['Actual IR', 'Predicted IR', 'Delta']\n",
    "    for col in format_cols_pct:\n",
    "        decile_summary[col] = (decile_summary[col] * 100).map('{:,.2f}%'.format)\n",
    "\n",
    "    display_cols = ['Policies', 'Actual IR', 'Predicted IR', 'Delta']\n",
    "    print(decile_summary[display_cols])\n",
    "    \n",
    "    overall_actual_ir = eval_df['actual_count'].sum() / eval_df['exposure'].sum()\n",
    "    overall_pred_ir = eval_df['predicted_count'].sum() / eval_df['exposure'].sum()\n",
    "    \n",
    "    print(f\"\\nOverall Actual IR:    {overall_actual_ir:.4%}\")\n",
    "    print(f\"Overall Predicted IR: {overall_pred_ir:.4%}\")\n",
    "    print(f\"Mean Absolute Delta:  {mean_abs_delta:.4f}%\") \n",
    "    \n",
    "    return mean_abs_delta\n",
    "\n",
    "# 5. Run Analysis & Report\n",
    "create_decile_analysis(y_train, pred_train, w_train, \"Train\")\n",
    "create_decile_analysis(y_test, pred_test, w_test, \"Test\")\n",
    "create_decile_analysis(y_oot, pred_oot, w_oot, \"OOT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb138397-add9-4804-87c7-c209a2808896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Feature-Level Analysis on OOT Set...\n",
      "\n",
      "--- Feature-Level Analysis: OOT (2025 Data) Set ---\n",
      "Analyzing top 10 features: ['experian_rank_final', 'variant_bracket_mapped', 'cc_group_ordinal', 'customer_age_group_ordinal', 'hit_flag_service', 'personal_loan_flag', 'embedded_red_flag', 'is_rsa', 'ex_showroom_price', 'corrected_body_type']\n",
      "\n",
      "Analyzing Feature: 'experian_rank_final'\n",
      "                     Policies Actual IR Predicted IR    Delta\n",
      "experian_rank_final                                          \n",
      "7.0000                   1887    36.66%       59.48%  -22.82%\n",
      "5.0000                   1459    41.33%       69.64%  -28.30%\n",
      "6.0000                   1251    38.69%       66.01%  -27.32%\n",
      " __NaN__                  443    33.33%       55.47%  -22.14%\n",
      "4.0000                    423    42.25%       68.46%  -26.20%\n",
      "3.0000                    329    42.46%       69.76%  -27.29%\n",
      "2.0000                     17    37.29%       83.31%  -46.02%\n",
      "\n",
      "Analyzing Feature: 'variant_bracket_mapped'\n",
      "                        Policies Actual IR Predicted IR    Delta\n",
      "variant_bracket_mapped                                          \n",
      " __NaN__                    5098    39.18%       65.39%  -26.21%\n",
      "3.0000                       317    34.90%       52.97%  -18.07%\n",
      "2.0000                       237    38.48%       63.84%  -25.36%\n",
      "1.0000                       157    33.49%       58.35%  -24.86%\n",
      "\n",
      "Analyzing Feature: 'cc_group_ordinal'\n",
      "                  Policies Actual IR Predicted IR    Delta\n",
      "cc_group_ordinal                                          \n",
      "1.0000                2099    38.55%       65.55%  -27.00%\n",
      "3.0000                1690    44.90%       63.81%  -18.91%\n",
      "2.0000                1190    30.10%       58.80%  -28.70%\n",
      "0.0000                 830    39.29%       71.13%  -31.84%\n",
      "\n",
      "Analyzing Feature: 'customer_age_group_ordinal'\n",
      "                            Policies Actual IR Predicted IR    Delta\n",
      "customer_age_group_ordinal                                          \n",
      "2.0000                          2284    34.27%       58.78%  -24.50%\n",
      "1.0000                          1758    43.64%       72.69%  -29.05%\n",
      "3.0000                           850    38.67%       56.40%  -17.73%\n",
      " __NaN__                         508    31.72%       55.62%  -23.90%\n",
      "0.0000                           409    51.97%       88.60%  -36.63%\n",
      "\n",
      "Analyzing Feature: 'hit_flag_service'\n",
      "                  Policies Actual IR Predicted IR    Delta\n",
      "hit_flag_service                                          \n",
      " __NaN__              3792    41.60%       67.56%  -25.96%\n",
      "0.0000                2017    33.43%       58.60%  -25.17%\n",
      "\n",
      "Analyzing Feature: 'personal_loan_flag'\n",
      "                    Policies Actual IR Predicted IR    Delta\n",
      "personal_loan_flag                                          \n",
      "0.0000                  2797    36.06%       61.16%  -25.10%\n",
      "1.0000                  2546    42.48%       69.64%  -27.16%\n",
      " __NaN__                 466    34.50%       55.64%  -21.13%\n",
      "\n",
      "Analyzing Feature: 'embedded_red_flag'\n",
      "                   Policies Actual IR Predicted IR    Delta\n",
      "embedded_red_flag                                          \n",
      " __NaN__               2631    33.45%       58.50%  -25.05%\n",
      "1.0000                 2203    47.56%       71.85%  -24.29%\n",
      "0.0000                  975    33.24%       63.82%  -30.57%\n",
      "\n",
      "Analyzing Feature: 'is_rsa'\n",
      "          Policies Actual IR Predicted IR    Delta\n",
      "is_rsa                                            \n",
      "1.0000        4428    39.25%       65.61%  -26.37%\n",
      "0.0000        1278    35.64%       60.31%  -24.67%\n",
      " __NaN__       103    56.25%       65.21%   -8.96%\n",
      "\n",
      "Analyzing Feature: 'ex_showroom_price'\n",
      "                   Policies Actual IR Predicted IR    Delta\n",
      "ex_showroom_price                                          \n",
      "1,550,000.0000          390    62.53%       67.86%   -5.33%\n",
      "999,000.0000             75    39.53%       62.31%  -22.78%\n",
      "1,069,000.0000           69    30.91%       54.67%  -23.76%\n",
      "999,900.0000             63    42.22%       70.13%  -27.91%\n",
      "3,256,000.0000           63    73.01%       80.19%   -7.18%\n",
      "1,450,000.0000           60    51.09%       66.21%  -15.12%\n",
      "1,499,000.0000           58    30.29%       64.13%  -33.84%\n",
      "1,219,000.0000           56    22.88%       51.37%  -28.49%\n",
      "1,749,000.0000           55    56.01%       69.10%  -13.09%\n",
      "1,240,000.0000           54    46.40%       72.31%  -25.91%\n",
      "1,349,000.0000           52    21.77%       53.50%  -31.73%\n",
      "925,000.0000             50    74.82%       67.24%    7.59%\n",
      "1,199,000.0000           49    43.19%       52.85%   -9.65%\n",
      "1,582,000.0000           45    14.90%       63.21%  -48.31%\n",
      "1,279,900.0000           43    45.58%       72.57%  -26.99%\n",
      "1,100,000.0000           40    28.24%       52.46%  -24.22%\n",
      "1,449,000.0000           38    30.84%       63.68%  -32.84%\n",
      "2,897,000.0000           38    42.92%       53.38%  -10.46%\n",
      "1,432,000.0000           38    16.40%       57.99%  -41.59%\n",
      "1,999,000.0000           37    22.34%       73.95%  -51.61%\n",
      "  ... (and 972 more values)\n",
      "\n",
      "Analyzing Feature: 'corrected_body_type'\n",
      "                     Policies Actual IR Predicted IR    Delta\n",
      "corrected_body_type                                          \n",
      "SUV                      4253    36.99%       62.12%  -25.13%\n",
      "HATCHBACK                 600    43.73%       72.56%  -28.82%\n",
      "__NaN__                   508    36.84%       64.34%  -27.50%\n",
      "SEDAN                     439    51.09%       76.00%  -24.92%\n",
      "VAN                         8    19.50%       43.79%  -24.29%\n",
      "Coupe                       1   134.19%       73.18%   61.01%\n",
      "Pickup Truck                0      nan%         nan%     nan%\n"
     ]
    }
   ],
   "source": [
    "# 6. Post-Model Analysis\n",
    "def create_feature_level_analysis(X_data, y_true, y_pred_count, exposure, model, set_name):\n",
    "    \"\"\"\n",
    "    Generates a feature-level analysis for top model features.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Feature-Level Analysis: {set_name} Set ---\")\n",
    "    \n",
    "    # Combine all data needed\n",
    "    eval_df = X_data.copy()\n",
    "    eval_df['actual_count'] = y_true\n",
    "    eval_df['predicted_count'] = y_pred_count\n",
    "    eval_df['exposure'] = exposure\n",
    "    \n",
    "    eval_df = eval_df[eval_df['exposure'] > 0].copy()\n",
    "    \n",
    "    if eval_df.empty:\n",
    "        print(f\"Warning: No data in {set_name} set for this analysis.\")\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        top_features = model.booster_.feature_name()[:10]\n",
    "        print(f\"Analyzing top 10 features: {top_features}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get feature names, skipping: {e}\")\n",
    "        return\n",
    "\n",
    "    for col in top_features:\n",
    "        if col not in eval_df.columns:\n",
    "            print(f\"Skipping '{col}', not in X_data.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nAnalyzing Feature: '{col}'\")\n",
    "        \n",
    "        feature_data = eval_df[col]\n",
    "        if feature_data.isnull().any():\n",
    "            if feature_data.dtype.name == 'category':\n",
    "                feature_data = feature_data.cat.add_categories('__NaN__').fillna('__NaN__')\n",
    "            else:\n",
    "                feature_data = feature_data.fillna('__NaN__')\n",
    "                \n",
    "        groups = eval_df.groupby(feature_data)\n",
    "        \n",
    "        summary_df = pd.DataFrame({\n",
    "            'Policies': groups.size(),\n",
    "            'Total Exposure': groups['exposure'].sum(),\n",
    "            'Actual Claims': groups['actual_count'].sum(),\n",
    "            'Predicted Claims': groups['predicted_count'].sum()\n",
    "        })\n",
    "        \n",
    "        summary_df['Actual IR'] = summary_df['Actual Claims'] / summary_df['Total Exposure']\n",
    "        summary_df['Predicted IR'] = summary_df['Predicted Claims'] / summary_df['Total Exposure']\n",
    "        summary_df['Delta'] = summary_df['Actual IR'] - summary_df['Predicted IR']\n",
    "        \n",
    "        summary_df = summary_df.sort_values(by='Policies', ascending=False)\n",
    "        \n",
    "        format_cols_pct = ['Actual IR', 'Predicted IR', 'Delta']\n",
    "        for pct_col in format_cols_pct:\n",
    "            summary_df[pct_col] = (summary_df[pct_col] * 100).map('{:,.2f}%'.format)\n",
    "\n",
    "        with pd.option_context('display.max_rows', 25):\n",
    "             print(summary_df[['Policies', 'Actual IR', 'Predicted IR', 'Delta']].head(20))\n",
    "        \n",
    "        if len(summary_df) > 20:\n",
    "            print(f\"  ... (and {len(summary_df) - 20} more values)\")\n",
    "\n",
    "# Run the analysis on the OOT set\n",
    "print(\"\\nRunning Feature-Level Analysis on OOT Set...\")\n",
    "create_feature_level_analysis(\n",
    "    X_oot, \n",
    "    y_oot, \n",
    "    pred_oot, \n",
    "    w_oot, \n",
    "    model, \n",
    "    \"OOT (2025 Data)\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
